eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 5*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 5*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
getwd()
setwd("C:/Users/Hp/Documents/GitHub/Batch_Means_Linear_Regression")
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 5*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 5*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 0.5*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 0.005*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
am <- c(am[am <= Iter-cutf],Iter-cutf)
ibs_jasa <- matrix(rep(0,nparm*(length(am)-1)),nrow=(length(am)-1),ncol=nparm)
tot_mean <- rep(0,nparm)
#Equal batch size smart batching (EBS)
for(k in 1 : (length(am)-1)){
strt_pt <- am[k]
end_pt <- am[k+1]-1
if(k==(length(am)-1)){  end_pt <- am[k+1]}
ibs_jasa[k,] <-  colSums(sg_ct[strt_pt : end_pt,])
tot_mean <- tot_mean + colSums(sg_ct[strt_pt:end_pt,])
}
tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
