tot_mean <- tot_mean/(Iter-cutf)
ibs_jasa_mean <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
for(k in 1 : (length(am)-1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k])*tot_mean)
if(k == (length(am) - 1)){
tmp_vl <- (ibs_jasa[k,]-(am[k+1]-am[k]+1)*tot_mean)}
ibs_jasa_mean <- ibs_jasa_mean + tmp_vl%*%t(tmp_vl)
}
ibs_jasa_mean <- ibs_jasa_mean/(Iter-cutf)
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_jasa_mean)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
rm(list=ls())
library(MASS)
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/grad_lin.R")
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#y <- mvrnorm(n,mu=rep(0,nparm),Sigma=diag(nparm)) %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
# JASA Online BM Estimators
for(m in 1:1000){
am[m] <- floor(m^(2/(1-alp)))
}
sg_ct
rm(list=ls())
library(MASS)
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/grad_lin.R")
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#noisy Observed Data
#y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
y <- mvrnorm(n,mu=rep(0,nparm),Sigma=diag(nparm)) %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
rm(list=ls())
library(MASS)
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/grad_lin.R")
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
#y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
x <- mvrnorm(n,mu = rep(0,nparm),Sigma = diag(nparm))
y <-  x%*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
sg_ct
ebs_mean <- ebs_batch_mean(sg_ct,alp)
rm(list=ls())
library(MASS)
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/grad_lin.R")
source("C:/Users/Hp/Documents/mcmc_with_rahul/simulations_linear_regression/ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
# x <- mvrnorm(n,mu = rep(0,nparm),Sigma = diag(nparm))
#y <-  x%*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
y <- mvrnorm(n,mu=rep(0,nparm),Sigma =diag(nparm) ) %*% parm + rnorm(n, mean = 0,sd = 1)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
y <- mvrnorm(n,mu=rep(0,nparm),Sigma =diag(nparm) ) %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
rm(list=ls())
library(MASS)
source("grad_lin.R")
source("ebs_batch_mean.R")
Rep <- 1
#Sample Size
n <- 5e5;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
install.packages("matrixcalc")
A <- matrix( c( 3, 5, 7, 2, 6, 4, 0, 2, 8 ), nrow=3, ncol=3, byrow=TRUE )
print( frobenius.norm( A ) )
library(matrixcalc)
A <- matrix( c( 3, 5, 7, 2, 6, 4, 0, 2, 8 ), nrow=3, ncol=3, byrow=TRUE )
print( frobenius.norm( A ) )
sqrt(sum(A^2))
A
A^2
rm(list=ls())
library(MASS)
library(matrixcalc)
source("grad_lin.R")
source("ebs_batch_mean.R")
source("ibs_jasa_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
#Iterations
Iter <- n;
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
cutf <- 1000 #Dropping initial Iterates of SGD
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm <- 1*diag(nparm)
decomp_sigm<- svd(sigm)
dig_sig <- decomp_sigm$d
v_mat <- decomp_sigm$u
sqrt_sig <- v_mat%*%diag(sqrt(dig_sig))%*%t(v_mat)
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
#noisy Observed Data
y <- x %*% parm + rnorm(n, mean = 0,sd = 1)
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(0, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - .5 * eta[i] * grad_lin(sg[i-1,],y[i],x[i,])
}
sg_ct <- sg[(cutf+1):Iter,]
ebs_mean <- ebs_batch_mean(sg_ct,alp)
ibs_mean <- ibs_jasa_mean(sg_ct,alp)
forb_ebs <- sqrt(sum((ebs_mean-diag(nparm))^2))
forb_ibs <- sqrt(sum((ibs_mean-diag(nparm))^2))
}
#Smart batching EBS
print(ebs_mean)
#JASA Chen batching
print(ibs_mean)
print(forb_ebs)
print(forb_ibs)
rm(list=ls())
library(MASS)
library(matrixcalc)
source("grad_lin.R")
source("ebs_batch_mean.R")
source("ibs_jasa_mean.R")
Rep <- 1
#Sample Size
n <- 5e4;
rm(list=ls())
library(MASS)
library(matrixcalc)
source("gradnt_log.R")
source("ebs_batch_mean.R")
source("ibs_jasa_mean.R")
source("sqrt_mat.R")
Rep <- 10
cutf <- 10000 #Dropping initial Iterates of SGD
#Sample Size
n <- 5e4+cutf;
#Confidence level
qlev <- 0.95
#Iterations
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
Iter <- n;
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm1 <- 1*diag(nparm)
sqrt_sig <- sqrt_mat(sigm1)
forb_ebs <- forb_ibs <- numeric(Rep)
volm_ebs <- volm_ibs <- numeric(Rep)
cover_ebs <- cover_ibs <- cover_orc <- numeric(Rep)
sigm <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
niter <- 1e4
#Sigma matrix estimate
for ( rp in 1:niter){
x1 <- rnorm(nparm)
sigm <- sigm + x1%*%t(x1)/(c((1+exp(t(x1)%*%parm))*(1+exp(-t(x1)%*%parm))*niter))
}
rm(list=ls())
library(MASS)
library(matrixcalc)
source("gradnt_log.R")
source("ebs_batch_mean.R")
source("ibs_jasa_mean.R")
source("sqrt_mat.R")
Rep <- 10
cutf <- 10000 #Dropping initial Iterates of SGD
#Sample Size
n <- 5e4+cutf;
#Confidence level
qlev <- 0.95
#Iterations
alp <- .51
nparm <- 5
parm <- rep(5,nparm)
am <- numeric(1000)
Iter <- n;
sg <- matrix(nrow = Iter, ncol = nparm);
sg_ct <- matrix(nrow = Iter - cutf, ncol = nparm)
#Iterates stored
# Sigma Matrix Stored with Square root
sigm1 <- 1*diag(nparm)
sqrt_sig <- sqrt_mat(sigm1)
forb_ebs <- forb_ibs <- numeric(Rep)
volm_ebs <- volm_ibs <- numeric(Rep)
cover_ebs <- cover_ibs <- cover_orc <- numeric(Rep)
sigm <- matrix(rep(0,nparm^2),nrow=nparm,ncol=nparm)
niter <- 1e4
#Sigma matrix estimate
for ( rp in 1:niter){
x1 <- rnorm(nparm)
sigm <- sigm + x1%*%t(x1)/(c((1+exp(t(x1)%*%parm))*(1+exp(-t(x1)%*%parm))*niter))
}
#1000  Replications to obtain stable mses
for(cn in 1 : Rep){
if(cn>=2){n <- n+cutf}
#Data Generated
x <- matrix(rnorm(n*nparm),nrow=n,ncol=nparm)
x <- x%*%sqrt_sig
p1 <- 1/(1 + exp(-x %*% parm))# Prob of success for 1 (1-p1 is for -1)
y<-2*rbinom(n,size=1,prob=p1)-1
#Learning Rate
eta <- numeric(Iter)
sg[1,] <- rep(4.5, nparm)
for(i in 2 : Iter){
eta[i] <- i^(-alp)
sg[i,] <- sg[i-1,] - 10 * eta[i] * gradnt_log(y[i],x[i,],sg[i-1,])
}
sg_ct <- sg[(cutf+1):Iter,]
asg <- colMeans(  sg_ct)
n <- n - cutf
#Variance-Covariance Estimate
ebs_mean <- ebs_batch_mean(sg_ct,alp)
ibs_mean <- ibs_jasa_mean(sg_ct,alp)
#Norm Difference
forb_ebs[cn] <- sqrt(sum((ebs_mean-sigm)^2))/sqrt(sum(sigm^2))
forb_ibs[cn] <- sqrt(sum((ibs_mean-sigm)^2))/sqrt(sum(sigm^2))
# Volume of the matrix
volm_ebs[cn] <- (det(ebs_mean))^(1/nparm)
volm_ibs[cn] <- (det(ibs_mean))^(1/nparm)
#critical value calculation
crt_val <- qchisq(qlev,df=nparm)
cover_ebs[cn] <- as.numeric(n*t(asg-parm)%*%solve(ebs_mean)%*%(asg-parm)<=crt_val)
cover_ibs[cn] <- as.numeric(n*t(asg-parm)%*%solve(ibs_mean)%*%(asg-parm)<=crt_val)
cover_orc[cn] <- as.numeric(n*t(asg-parm)%*%solve(sigm)%*%(asg-parm)<=crt_val)
}
rm(list=ls())
source("ibs_lng.R")
alp <- .51
sq <- seq(5,20,by=5)
leng_ibs <- matrix(nrow=5,ncol=length(sq))
leng_ebs <- matrix(nrow=5,ncol=length(sq))
for( k in 1:length(sq)){
nparm <- sq[k]
leng_ibs[1,k] <- ibs_lng(nparm,cns=0.01,alp)
leng_ibs[2,k] <- ibs_lng(nparm,cns=0.1,alp)
leng_ibs[3,k] <- ibs_lng(nparm,cns=1,alp)
leng_ibs[4,k] <- ibs_lng(nparm,cns=2,alp)
leng_ibs[5,k] <- ibs_lng(nparm,cns=5,alp)
#Number of batches in IBS scheme
}
setwd("C:/Users/Hp/Documents/GitHub/Batch_Means_Online/check_value_constant")
rm(list=ls())
setwd("C:/Users/Hp/Documents/GitHub/Batch_Means_Online/check_value_constant")
source("ibs_lng.R")
alp <- .51
sq <- seq(5,20,by=5)
leng_ibs <- matrix(nrow=5,ncol=length(sq))
leng_ebs <- matrix(nrow=5,ncol=length(sq))
for( k in 1:length(sq)){
nparm <- sq[k]
leng_ibs[1,k] <- ibs_lng(nparm,cns=0.01,alp)
leng_ibs[2,k] <- ibs_lng(nparm,cns=0.1,alp)
leng_ibs[3,k] <- ibs_lng(nparm,cns=1,alp)
leng_ibs[4,k] <- ibs_lng(nparm,cns=2,alp)
leng_ibs[5,k] <- ibs_lng(nparm,cns=5,alp)
#Number of batches in IBS scheme
}
leng_ibs
rm(list=ls())
setwd("C:/Users/Hp/Documents/GitHub/Batch_Means_Online/check_value_constant")
source("ibs_lng.R")
alp <- .51
sq <- seq(5,100,by=5)
leng_ibs <- matrix(nrow=5,ncol=length(sq))
leng_ebs <- matrix(nrow=5,ncol=length(sq))
for( k in 1:length(sq)){
nparm <- sq[k]
leng_ibs[1,k] <- ibs_lng(nparm,cns=0.01,alp)
leng_ibs[2,k] <- ibs_lng(nparm,cns=0.1,alp)
leng_ibs[3,k] <- ibs_lng(nparm,cns=1,alp)
leng_ibs[4,k] <- ibs_lng(nparm,cns=2,alp)
leng_ibs[5,k] <- ibs_lng(nparm,cns=5,alp)
#Number of batches in IBS scheme
}
leng_ibs
leng_ibs
leng_ibs
